{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Sequencing Success: A Hands-On Workshop in Deep Learning for Sequence-to-Sequence Models\n",
        "Moratuwa Engineering Research Conference 2023 (MERCon 2023) is the 9th international conference organized by the Engineering Research Unit at the University of Moratuwa. As part of MERCon 2023, we are hosting a Hands-On Workshop on Deep Learning for Sequence-to-Sequence Models. This workshop spans four hours and is divided into four one-hour sessions, covering the following topics:\n",
        "- Introduction to Sequence-to-Sequence Learning\n",
        "- Sequence-to-Sequence Learning with Recurrent Neural Networks (RNNs)\n",
        "- Sequence-to-Sequence Learning with Encoder-Decoder Models\n",
        "- Sequence-to-Sequence Learning with Encoder-Decoder Models and Attention Mechanisms\n",
        "\n",
        "This notebook is prepared for session **Sequence-to-Sequence Learning with Recurrent Neural Networks**.\n",
        "\n",
        "All rights reserved.\n",
        "\n",
        "Authors:\n",
        "1.   Dr.T.Uthayasanker ([rtuthaya.lk](https://rtuthaya.lk))\n",
        "2.   Mr.S.Braveenan ([Braveenan Sritharan](https://www.linkedin.com/in/braveenan-sritharan/))\n",
        "\n",
        "[For more information - MERCon 2023](https://mercon.uom.lk)"
      ],
      "metadata": {
        "id": "txeX5oBgtgwh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixdBZoDD7uI2"
      },
      "source": [
        "#A Simple Seq2Seq Problem 2: The reverse sentence problem\n",
        "In this simple Seq2Seq problem, we are provided with a **parallel dataset** comprising two sentences, X (input) and y (output). In this scenario, the output sentence, y[i], is constructed by reversing the order of the words in the input sentence, X[i]. To illustrate, consider an example where the **input sentence X[i]** has a length of 6, such as:\n",
        "\n",
        "X[i] = **he ate apple**\n",
        "\n",
        "The corresponding **output sentence, y[i]**, would be:\n",
        "\n",
        "y[i] = **apple ate he**\n",
        "\n",
        "This problem serves as a foundational example of a Sequence-to-Sequence (Seq2Seq) task, where the objective is to learn to reverse sentences effectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8795rUvP1qK"
      },
      "outputs": [],
      "source": [
        "#@title Import Libraries\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras import Input\n",
        "from keras.layers import RepeatVector, Dense, SimpleRNN, GRU, LSTM, TimeDistributed\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.utils import plot_model\n",
        "from keras.models import Sequential, Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPQZIOaAjcoU"
      },
      "source": [
        "#Auxiliary functions\n",
        "\n",
        "Certainly! Here's a more concise introduction to the key functions in the code snippet:\n",
        "1. **generate_text_sequence(length, word_array)**: Generates a random text sequence of a given length using words from an array.\n",
        "2. **one_hot_encode_text(text_sequence, word_array)**: Converts a text sequence into one-hot encoded vectors using a word array.\n",
        "3. **one_hot_decode_text(encoded_seq, word_array)**: Decodes a one-hot encoded sequence back into its original text form.\n",
        "4. **get_reversed_pairs(time_steps, word_array, verbose=False)**: Generates pairs of random sequences and their reversals, one-hot encodes them, and returns them for training.\n",
        "5. **create_dataset(train_size, test_size, time_steps, word_array, verbose=False)**: Creates training and testing datasets by generating reversed pairs.\n",
        "6. **train_test(model, X_train, y_train, X_test, y_test, epochs=100, verbose=0)**: Trains a neural network model, evaluates it, and returns the model and training history.\n",
        "7. **visualize_history(history)**: Visualizes the training history, showing accuracy and loss over epochs.\n",
        "8. **check_samples(model, X_test, y_test, word_array, num_samples=10)**: Checks the model's performance on a set of sample sequences from the testing data.\n",
        "\n",
        "These functions collectively support the process of training and evaluating a neural network for a sentence reversal task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtqfmtPdK5by"
      },
      "outputs": [],
      "source": [
        "#@title Function to generate a text sequence\n",
        "# generate sequence\n",
        "def generate_text_sequence(length, word_array):\n",
        "    word_sequence = [random.choice(word_array) for _ in range(length)]\n",
        "    text_sequence = ' '.join(word_sequence)\n",
        "    return text_sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43n268VmZH21"
      },
      "outputs": [],
      "source": [
        "#@title Function to encode and decode text sequence\n",
        "# one hot encode sequence\n",
        "def one_hot_encode_text(text_sequence, word_array):\n",
        "    encoding = []\n",
        "    for word in text_sequence.split():\n",
        "        vector = [0] * len(word_array)\n",
        "        if word in word_array:\n",
        "            vector[word_array.index(word)] = 1\n",
        "        encoding.append(vector)\n",
        "    return np.array(encoding)\n",
        "\n",
        "# decode a one hot encoded string\n",
        "def one_hot_decode_text(encoded_seq, word_array):\n",
        "    decoded_sequence = [word_array[np.argmax(vector)] for vector in encoded_seq]\n",
        "    return ' '.join(decoded_sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvbJK74jUt1y"
      },
      "outputs": [],
      "source": [
        "#@title Function to generate reverse pair dataset\n",
        "# create one reverse pair\n",
        "def get_reversed_pairs(time_steps,word_array,verbose= False):\n",
        "\t\t# generate random sequence\n",
        "\t\tsequence_in = generate_text_sequence(time_steps, word_array)\n",
        "\t\tsequence_out = ' '.join(sequence_in.split()[::-1])\n",
        "\n",
        "\t\t# one hot encode\n",
        "\t\tX = one_hot_encode_text(sequence_in, word_array)\n",
        "\t\ty = one_hot_encode_text(sequence_out, word_array)\n",
        "\t\t# reshape as 3D\n",
        "\t\tX = X.reshape((1, X.shape[0], X.shape[1]))\n",
        "\t\ty = y.reshape((1, y.shape[0], y.shape[1]))\n",
        "\n",
        "\t\tif(verbose):\n",
        "\t\t\tprint('\\nSample X and y')\n",
        "\t\t\tprint('\\nIn raw format:')\n",
        "\t\t\tprint('X[0]=%s, y[0]=%s' % (one_hot_decode_text(X[0], word_array), one_hot_decode_text(y[0], word_array)))\n",
        "\t\t\tprint('\\nIn one_hot_encoded format:')\n",
        "\t\t\tprint('X[0]=%s' % (X[0]))\n",
        "\t\t\tprint('y[0]=%s' % (y[0]))\n",
        "\t\treturn X,y\n",
        "\n",
        "# create final dataset\n",
        "def create_dataset(train_size, test_size, time_steps,word_array, verbose= False):\n",
        "\t\tpairs = [get_reversed_pairs(time_steps,word_array) for _ in range(train_size)]\n",
        "\t\tpairs=np.array(pairs).squeeze()\n",
        "\t\tX_train = pairs[:,0]\n",
        "\t\ty_train = pairs[:,1]\n",
        "\t\tpairs = [get_reversed_pairs(time_steps,word_array) for _ in range(test_size)]\n",
        "\t\tpairs=np.array(pairs).squeeze()\n",
        "\t\tX_test = pairs[:,0]\n",
        "\t\ty_test = pairs[:,1]\n",
        "\n",
        "\t\tif(verbose):\n",
        "\t\t\tprint('\\nGenerated sequence datasets as follows')\n",
        "\t\t\tprint('X_train.shape: ', X_train.shape,'y_train.shape: ', y_train.shape)\n",
        "\t\t\tprint('X_test.shape: ', X_test.shape,'y_test.shape: ', y_test.shape)\n",
        "\n",
        "\t\treturn X_train, y_train, X_test, \ty_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KdZwSEM9bxiJ"
      },
      "outputs": [],
      "source": [
        "#@title Function to train and evaluate model\n",
        "def train_test(model, X_train, y_train , X_test, y_test, epochs=100, verbose=0):\n",
        "    # patient early stopping\n",
        "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
        "\n",
        "    # train model\n",
        "    print('training for ',epochs,' epochs begins with validation_split= 0.1 & EarlyStopping(monitor= val_loss, patience=20)....')\n",
        "    history = model.fit(X_train, y_train, validation_split=0.1, epochs=epochs, verbose=verbose, callbacks=[es])\n",
        "    print(epochs,' epoch training finished...')\n",
        "\n",
        "    # evaluate the model\n",
        "    _, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
        "    _, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "    print('\\nPREDICTION ACCURACY (%):')\n",
        "    print('Train: %.3f, Test: %.3f' % (train_acc*100, test_acc*100))\n",
        "\n",
        "    return model, history.history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzUal4wM53Je"
      },
      "outputs": [],
      "source": [
        "#@title Function to visualize loss and accuracy\n",
        "def visualize_history(history):\n",
        "\t# summarize history for accuracy\n",
        "\tplt.plot(history['accuracy'])\n",
        "\tplt.plot(history['val_accuracy'])\n",
        "\tplt.ylabel('accuracy')\n",
        "\tplt.xlabel('epoch')\n",
        "\tplt.legend(['train', 'val'], loc='upper left')\n",
        "\tplt.show()\n",
        "\t# summarize history for loss\n",
        "\tplt.plot(history['loss'])\n",
        "\tplt.plot(history['val_loss'])\n",
        "\tplt.ylabel('loss')\n",
        "\tplt.xlabel('epoch')\n",
        "\tplt.legend(['train', 'val'], loc='upper left')\n",
        "\tplt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QijXbREH9EFs"
      },
      "outputs": [],
      "source": [
        "#@title Function to check some examples\n",
        "def check_samples(model, X_test, y_test, word_array, num_samples=10):\n",
        "    sample_indices = random.sample(range(len(X_test)), num_samples)\n",
        "\n",
        "    for id in sample_indices:\n",
        "        X, y = X_test[id], y_test[id]\n",
        "        X = np.expand_dims(X, axis=0)\n",
        "        y = np.expand_dims(y, axis=0)\n",
        "        yhat = model.predict(X, verbose=0)\n",
        "        print(f\"Input: {one_hot_decode_text(X[0], word_array)} \\nExpected: {one_hot_decode_text(y[0], word_array)} \\nPredicted: {one_hot_decode_text(yhat[0], word_array)} \\n{np.array_equal(one_hot_decode_text(y[0], word_array), one_hot_decode_text(yhat[0], word_array))}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrmC-Z9AjvEa"
      },
      "source": [
        "#Create reverse sentence dataset\n",
        "\n",
        "\n",
        "This code snippet is designed to generate a dataset for the **Reverse Sentence Task**. In this task, each data point consists of a sentence and its reversed counterpart. The essential parameters include the input sequence length (n_timesteps_in), the number of unique words (n_features), the size of the training dataset (train_size), and the size of the testing dataset (test_size).\n",
        "\n",
        "The code accomplishes the following:\n",
        "1. It generates a random sentence and its reversed version, one-hot encodes them, and optionally displays sample pairs of sentences to illustrate the dataset structure.\n",
        "2. The code then creates training and testing datasets by generating pairs of random sentences and their reversals. The dataset sizes are determined by the parameters train_size and test_size.\n",
        "\n",
        "This dataset is a crucial component for training and evaluating models for the **Reverse Sentence Task**, which is a common problem in natural language processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jck3vi7clwt"
      },
      "outputs": [],
      "source": [
        "word_array = [\"apple\", \"banana\", \"cherry\", \"orange\", \"strawberry\",\n",
        "             \"carrot\", \"broccoli\", \"potato\", \"tomato\", \"cucumber\",\n",
        "             \"rose\", \"tulip\", \"daisy\", \"lily\", \"sunflower\",\n",
        "             \"red\", \"blue\", \"green\", \"yellow\", \"purple\",\n",
        "             \"Colombo\", \"London\", \"Paris\", \"Tokyo\", \"Sydney\",\n",
        "             \"car\", \"bus\", \"bicycle\", \"train\", \"motorcycle\",\n",
        "             \"guitar\", \"piano\", \"violin\", \"trumpet\", \"flute\",\n",
        "             \"beach\", \"mountain\", \"park\", \"desert\", \"island\",\n",
        "             \"book\", \"computer\", \"chair\", \"table\", \"lamp\",\n",
        "             \"dog\", \"cat\", \"bird\", \"elephant\", \"lion\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pN3fQDMvpSar"
      },
      "outputs": [],
      "source": [
        "#@title Generating dataset\n",
        "# Default configuration parameters\n",
        "n_timesteps_in = 6\n",
        "n_features = len(word_array)\n",
        "train_size = 20000\n",
        "test_size = 200\n",
        "\n",
        "# Generate random sequence using specified parameters\n",
        "X, y = get_reversed_pairs(n_timesteps_in, word_array, verbose=True)\n",
        "\n",
        "# Generate datasets using specified parameters\n",
        "X_train, y_train, X_test, y_test = create_dataset(train_size, test_size, n_timesteps_in, word_array, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTkpnuTWj31H"
      },
      "source": [
        "#1. Multi-Layer Perceptron network model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2ye6TxsuQMl"
      },
      "outputs": [],
      "source": [
        "#@title Create Multi-Layer Perceptron network model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h21sW8UNvodJ"
      },
      "outputs": [],
      "source": [
        "#@title Train and Evaluate Multi-Layer Perceptron network model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9l1hwiX6Evp"
      },
      "outputs": [],
      "source": [
        "#@title Visualize training and validation Multi-Layer Perceptron network model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IlrtHMe-Dqo"
      },
      "outputs": [],
      "source": [
        "#@title Check random samples Multi-Layer Perceptron network model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfNb5y2LpAvL"
      },
      "source": [
        "#2. Recurrent Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7wvqYV3q0wN"
      },
      "source": [
        "#2.1. Simple Recurrent Neural Network model (RNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVpQ5_4Iq8xd"
      },
      "outputs": [],
      "source": [
        "#@title Create simple RNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqTCshZdrO9Y"
      },
      "outputs": [],
      "source": [
        "#@title Train and Evaluate simple RNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xj914mVrXK9"
      },
      "outputs": [],
      "source": [
        "#@title Visualize training and validation simple RNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kT2-MN85rhIC"
      },
      "outputs": [],
      "source": [
        "#@title Check random samples simple RNN model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAipAWERsWj2"
      },
      "source": [
        "#2.2. Gated Recurrent Units model (GRU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4w0norWPsa3h"
      },
      "outputs": [],
      "source": [
        "#@title Create GRU model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJ74bIzyslOW"
      },
      "outputs": [],
      "source": [
        "#@title Train and Evaluate GRU model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UM_0Pg-Esyws"
      },
      "outputs": [],
      "source": [
        "#@title Visualize training and validation GRU model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BYvKmLrs48G"
      },
      "outputs": [],
      "source": [
        "#@title Check random samples GRU model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxocN_cml1qL"
      },
      "source": [
        "#2.3. Long Short-Term Memory model (LSTM)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSzsfNpdboh5"
      },
      "outputs": [],
      "source": [
        "#@title Create LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kSRaj3ueT2p"
      },
      "outputs": [],
      "source": [
        "#@title Train and Evaluate LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JEokw2fyeii7"
      },
      "outputs": [],
      "source": [
        "#@title Visualize training and validation LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SORYAE5Xep6l"
      },
      "outputs": [],
      "source": [
        "#@title Check random samples LSTM model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpuHp6H24UGy"
      },
      "source": [
        "#3. Information Sharing between RNN Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRnWZTXWk72S"
      },
      "source": [
        "#3.1. LSTM model - Only last hidden state\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5drjH3VPCO7"
      },
      "outputs": [],
      "source": [
        "#@title Create LSTM model with only last hidden state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b81glRgNSl36"
      },
      "outputs": [],
      "source": [
        "#@title Train and Evaluate LSTM model with only last hidden state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nngiLf2TCyD"
      },
      "outputs": [],
      "source": [
        "#@title Visualize training and validation LSTM model with only last hidden state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWu1tBwFTMIB"
      },
      "outputs": [],
      "source": [
        "#@title Check random samples LSTM model with only last hidden state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9yetwEqmLEw"
      },
      "source": [
        "#3.2. LSTM model - Last hidden state and last cell state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktniM-BYlp3I"
      },
      "outputs": [],
      "source": [
        "#@title Create LSTM model with last hidden state and last cell state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JuS31gEmmrK1"
      },
      "outputs": [],
      "source": [
        "#@title Train and Evaluate LSTM model with last hidden state and last cell state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCmnS_C3m5Gb"
      },
      "outputs": [],
      "source": [
        "#@title Visualize training and validation LSTM model with last hidden state and last cell state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BJYFmdUm-oP"
      },
      "outputs": [],
      "source": [
        "#@title Check random samples LSTM model with last hidden state and last cell state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnMWAHm7ml1Z"
      },
      "source": [
        "#3.3. LSTM model - All hidden states and last cell state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMYba7c-nGLp"
      },
      "outputs": [],
      "source": [
        "#@title Create LSTM model with all hidden states and last cell state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLfnPZEcn4RH"
      },
      "outputs": [],
      "source": [
        "#@title Train and Evaluate LSTM model with all hidden states and last cell state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YlaAQhy7oD2e"
      },
      "outputs": [],
      "source": [
        "#@title Visualize training and validation LSTM model with all hidden states and last cell state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rq_qoyzGoKJe"
      },
      "outputs": [],
      "source": [
        "#@title Check random samples LSTM model with all hidden states and last cell state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5J1pD2ceqkB"
      },
      "source": [
        "#Reference\n",
        "1. https://www.muratkarakaya.net/2022/11/seq2seq-learning-tutorial-series.html\n",
        "2. https://deeplearningmath.org/sequence-models.html\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "ixdBZoDD7uI2",
        "nPQZIOaAjcoU",
        "XrmC-Z9AjvEa",
        "UTkpnuTWj31H",
        "UfNb5y2LpAvL",
        "o7wvqYV3q0wN",
        "eAipAWERsWj2",
        "PxocN_cml1qL",
        "WpuHp6H24UGy",
        "hRnWZTXWk72S",
        "z9yetwEqmLEw",
        "DnMWAHm7ml1Z"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}